@article{reisDevelopingDockerDockerCompose2022,
  title = {Developing {{Docker}} and {{Docker-Compose Specifications}}: {{A Developers}}’ {{Survey}}},
  shorttitle = {Developing {{Docker}} and {{Docker-Compose Specifications}}},
  author = {Reis, David and Piedade, Bruno and Correia, Filipe F. and Dias, João Pedro and Aguiar, Ademar},
  date = {2022},
  journaltitle = {IEEE Access},
  volume = {10},
  pages = {2318--2329},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3137671},
  abstract = {Cloud computing and Infrastructure-as-Code (IaC), supported by technologies such as Docker, have shaped how many software systems are built and deployed. Previous research has identified typical issues for some types of IaC specification but not why they come to be, or they have delved into collaboration aspects but not into technical ones. This work aims to characterize the activities around two particular kinds of IaC specification—Dockerfiles and docker-compose.yml files. We seek to know how they can be better supported and therefore study also what approaches and tools practitioners employ. We used an online questionnaire to gather data. The first part of the study reached 68 graduate students from a study program on informatics engineering, and the second one 120 professional software developers. The results show that most of the activities of the process of developing a Dockerfile are perceived as time-consuming, especially when the respondents are beginners with this technology. We also found that solving issues using trial-and-error approaches is very common and that many developers do not use ancillary tools to support the development of Dockerfiles and docker-compose.yml files.},
  eventtitle = {{{IEEE Access}}},
  keywords = {cloud computing,Cloud computing,Docker,docker-compose,Measurement,Open source software,orchestration,Social networking (online),Software development management,survey},
  file = {/home/mechjm/Zotero/storage/2KBWTD9N/Reis et al_2022_Developing Docker and Docker-Compose Specifications.pdf;/home/mechjm/Zotero/storage/SGTVB44Y/9658534.html}
}

@article{agadakosLargescaleDebloatingBinary2020,
  title = {Large-Scale {{Debloating}} of {{Binary Shared Libraries}}},
  author = {Agadakos, Ioannis and Demarinis, Nicholas and Jin, Di and Williams-King, Kent and Alfajardo, Jearson and Shteinfeld, Benjamin and Williams-King, David and Kemerlis, Vasileios P. and Portokalidis, Georgios},
  date = {2020-12-17},
  journaltitle = {Digital Threats: Research and Practice},
  shortjournal = {Digital Threats},
  volume = {1},
  number = {4},
  pages = {19:1--19:28},
  issn = {2692-1626},
  doi = {10.1145/3414997},
  url = {https://doi.org/10.1145/3414997},
  urldate = {2022-12-20},
  abstract = {Developers nowadays have access to an arsenal of toolkits and libraries for rapid application prototyping. However, when an application loads a library, the entirety of that library’s code is mapped into the process address space, even if only a single function is actually needed. The unused portion is bloat that can negatively impact software defenses by unnecessarily inflating their overhead or increasing the attack surface. In this article, we investigate whether debloating is possible and practical at the binary level. To this end, we present Nibbler: a system that identifies and erases unused functions within dynamic shared libraries. Nibbler works in tandem with defenses such as continuous code re-randomization and control-flow integrity, enhancing them without incurring additional run-time overhead. We developed and tested a prototype of Nibbler on x86-64 Linux; Nibbler reduces the size of shared libraries and the number of available functions, for real-world binaries and the SPEC CINT2006 suite, by up to 56\% and 82\%, respectively. We also demonstrate that Nibbler benefits defenses by showing that: (i) it improves the deployability of a continuous re-randomization system for binaries, namely, Shuffler, by increasing its efficiency by 20\%, and (ii) it improves certain fast but coarse and context-insensitive control-flow integrity schemes by reducing the number of gadgets reachable through indirect branch instructions by 75\% and 49\%, on average. Last, we apply Nibbler on ≈30K C/C++ binaries and ≈5K unique dynamic shared libraries (i.e., almost the complete set of the Debian sid distribution), as well as on nine official Docker images (with millions of downloads in Docker Hub), reporting entrancing findings regarding code bloat at large.},
  keywords = {Code debloating,software security,static binary analysis},
  file = {/home/mechjm/Zotero/storage/CJJST43C/Agadakos et al_2020_Large-scale Debloating of Binary Shared Libraries.pdf}
}

@article{ahmedSecurityAnalysisCode2022,
  title = {Security {{Analysis}} of {{Code Bloat}} in {{Machine Learning Systems}}},
  author = {Ahmed, Fahmi Abdulqadir and Fatih, Dyako},
  date = {2022},
  url = {https://hdl.handle.net/20.500.12380/305124},
  urldate = {2022-12-22},
  abstract = {Code bloat is a significant issue in modern software systems as they continue to increase in size and complexity. Furthermore, with the widespread adoption of containerized applications, there is an abundance of unneeded packages that suffer from a wide range of vulnerabilities. In this thesis, we analyze the prevalence of security vulnerabilities in containers used for Machine Learning (ML) systems. We consider two popular ML frameworks, namely, PyTorch and TensorFlow. Making use of container scanning tools, we observed over 100 Common Vulnerabilities and Exposures (CVE) in the tested containers. Our experiments show that debloating using Cimplifier leads to a reduction in the image sizes of up to 49\% and a reduction of vulnerabilities of at least 87\%. The majority of the removed CVEs can be attributed to the removal of bloat specific to redundant parts of the containers’ installed OS packages. A smaller portion of the CVEs detected in the Python packages were removed by Cimplifier.},
  langid = {english},
  file = {/home/mechjm/Zotero/storage/K4QDXFVW/Ahmed_Fatih_2022_Security Analysis of Code Bloat in Machine Learning Systems.pdf}
}

@article{azumaEmpiricalStudySelfadmitted2022,
  title = {An Empirical Study on Self-Admitted Technical Debt in {{Dockerfiles}}},
  author = {Azuma, Hideaki and Matsumoto, Shinsuke and Kamei, Yasutaka and Kusumoto, Shinji},
  date = {2022-01-31},
  journaltitle = {Empirical Software Engineering},
  shortjournal = {Empir Software Eng},
  volume = {27},
  number = {2},
  pages = {49},
  issn = {1573-7616},
  doi = {10.1007/s10664-021-10081-7},
  url = {https://doi.org/10.1007/s10664-021-10081-7},
  urldate = {2022-12-17},
  abstract = {In software development, ad hoc solutions that are intentionally implemented by developers are called self-admitted technical debt (SATD). Because the existence of SATD spreads poor implementations, it is necessary to remove it as soon as possible. Meanwhile, container virtualization has been attracting attention in recent years as a technology to support infrastructure such as servers. Currently, Docker is the de facto standard for container virtualization. In Docker, a file describing how to build a container (Dockerfile) is a set of procedural instructions; thus, it can be considered as a kind of source code. Moreover, because Docker is a relatively new technology, there are few developers who have accumulated good or bad practices for building Docker container. Hence, it is likely that Dockerfiles contain many SATDs, as is the case with general programming language source code analyzed in previous SATD studies. The goal of this paper is to categorize SATDs in Dockerfiles and to share knowledge with developers and researchers. To achieve this goal, we conducted a manual classification for SATDs in Dockerfile. We found that about 3.0\% of the comments in Dockerfile are SATD. In addition, we have classified SATDs into five classes and eleven subclasses. Among them, there are some SATDs specific to Docker, such as SATDs for version fixing and for integrity check. The three most common classes of SATD were related to lowering maintainability, testing, and defects.},
  langid = {english},
  keywords = {Container virtualization,Docker,Infrastructure as code (IaC),Self-admitted technical debt,Technical debt},
  file = {/home/mechjm/Zotero/storage/LM8PUUCR/Azuma et al_2022_An empirical study on self-admitted technical debt in Dockerfiles.pdf}
}

@inproceedings{benniSupportingMicroservicesDeployment2018,
  title = {Supporting Micro-Services Deployment in a Safer Way: A Static Analysis and Automated Rewriting Approach},
  shorttitle = {Supporting Micro-Services Deployment in a Safer Way},
  booktitle = {Proceedings of the 33rd {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Benni, Benjamin and Mosser, Sébastien and Collet, Philippe and Riveill, Michel},
  date = {2018-04-09},
  series = {{{SAC}} '18},
  pages = {1706--1715},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3167132.3167314},
  url = {https://doi.org/10.1145/3167132.3167314},
  urldate = {2022-12-18},
  abstract = {The SOA ecosystem has drastically evolved since its childhood in the early 2000s. From monolithic services, micro-services now cooperate together in ultra-large scale systems. In this context, there is a tremendous need to deploy frequently new services, or new version of existing services. Container-based technologies (e.g., Docker) emerged recently to tool such deployments, promoting a black-box reuse mechanism to support off-the-shelf deployments. Unfortunately, from the service deployment point of view, such form of black-box reuse prevent to ensure what is really shipped inside the container with the service to deploy. In this paper, we propose a formalism to model and statically analyze service deployment artifacts based on state of the art deployment platforms. The static analysis mechanism leverages the hierarchy of deployment descriptors to verify a given deployment, as well as rewrite it to automatically fix common errors. The approach is validated through the automation of the guidelines provided by the user community associated to the reference Docker engine, and the analysis of 20,000 real deployment descriptors (hosted on GitHub).},
  isbn = {978-1-4503-5191-1},
  keywords = {container,docker,microservice,static analysis},
  file = {/home/mechjm/Zotero/storage/MI869HHA/Benni et al_2018_Supporting micro-services deployment in a safer way.pdf}
}

@article{burnettVisualProgramming1999,
  title = {Visual {{Programming}}},
  author = {Burnett, Margaret M.},
  date = {1999},
  journaltitle = {Encyclopedia of Electrical and Electronics Engineering (John G. Webster, ed.), John Wiley \& Sons Inc., New York},
  pages = {13},
  url = {https://www.cs.auckland.ac.nz/courses/compsci732s1c/archive/2005/lectures/WhatIsVP.pdf},
  file = {/home/mechjm/Zotero/storage/3QWXS3JL/WhatIsVP.pdf}
}

@inproceedings{caturanoExploitWP2DockerPlatformAutomating2022,
  title = {{{ExploitWP2Docker}}: A {{Platform}} for {{Automating}} the {{Generation}} of {{Vulnerable WordPress Environments}} for {{Cyber Ranges}}},
  shorttitle = {{{ExploitWP2Docker}}},
  booktitle = {2022 {{International Conference}} on {{Electrical}}, {{Computer}} and {{Energy Technologies}} ({{ICECET}})},
  author = {Caturano, Francesco and d’ Ambrosio, Nicola and Perrone, Gaetano and Previdente, Luigi and Romano, Simon Pietro},
  options = {useprefix=true},
  date = {2022-07},
  pages = {1--7},
  doi = {10.1109/ICECET55527.2022.9872859},
  abstract = {A cyber range is a realistic simulation of an organization’s network infrastructure, commonly used for cyber security training purposes. It provides a safe environment to assess competencies in both offensive and defensive techniques. An important step during the realization of a cyber range is the generation of vulnerable machines. This step is challenging and requires a laborious manual configuration. Several works aim to reduce this overhead, but the current state-of-the-art focuses on generating network services without considering the effort required to build vulnerable environments for web applications. A cyber range should represent a real system, and nowadays, almost all the companies develop their company site by using WordPress, a common Content Management System (CMS), which is also one of the most critical attackers’ entry points. The presented work proposes an approach to automatically create and conFigure vulnerable WordPress applications by using the information presented in public exploits. Our platform automatically extracts information from the most well-known publicly available exploit database in order to generate and conFigure vulnerable environments. The container-based virtualization is used to generate lightweight and easily deployable infrastructures. A final evaluation highlights promising results regarding the possibility of automating the generation of vulnerable environments through our approach.},
  eventtitle = {2022 {{International Conference}} on {{Electrical}}, {{Computer}} and {{Energy Technologies}} ({{ICECET}})},
  keywords = {Browsers,Companies,container-based virtualization,Containers,Content management,Content Management Systems,Cyber ranges,Databases,Manuals,security,Training},
  file = {/home/mechjm/Zotero/storage/FU5GPJMN/Caturano et al_2022_ExploitWP2Docker.pdf;/home/mechjm/Zotero/storage/XZHZBDYU/9872859.html}
}

@software{ContainerStructureTests2022,
  title = {Container {{Structure Tests}}},
  date = {2022-12-30T12:34:50Z},
  origdate = {2017-10-23T21:57:40Z},
  url = {https://github.com/GoogleContainerTools/container-structure-test},
  urldate = {2022-12-30},
  abstract = {validate the structure of your container images},
  organization = {{GoogleContainerTools}}
}

@article{doanDAVSDockerfileAnalysis2022,
  title = {{{DAVS}}: {{Dockerfile Analysis}} for {{Container Image Vulnerability Scanning}}},
  shorttitle = {{{DAVS}}},
  author = {Doan, Thien-Phuc and Jung, Souhwan},
  date = {2022},
  journaltitle = {Computers, Materials \& Continua},
  shortjournal = {cmc},
  volume = {72},
  number = {1},
  pages = {1699--1711},
  publisher = {{Tech Science Press}},
  issn = {1546-2218, 1546-2218},
  doi = {10.32604/cmc.2022.025096},
  url = {https://www.techscience.com/cmc/v72n1/46926},
  urldate = {2022-12-18},
  abstract = {Container technology plays an essential role in many Information and Communications Technology (ICT) systems. However, containers face a diversity of threats caused by vulnerable packages within container images. Previous vulnerability scanning solutions for container images are inadequate. These solutions entirely depend on the information extracted from package managers. As a result, packages installed directly from the source code compilation, or packages downloaded from the repository, etc., are ignored. We introduce DAVS–A Dockerfile analysis-based vulnerability scanning framework for OCI-based container images to deal with the limitations of existing solutions. DAVS performs static analysis using file extraction based on Dockerfile information to obtain the list of Potentially Vulnerable Files (PVFs). The PVFs are then scanned to figure out the vulnerabilities in the target container image. The experimental shows the outperform of DAVS on detecting Common Vulnerabilities and Exposures (CVE) of 10 known vulnerable images compared to Clair– the most popular container image scanning project. Moreover, DAVS found that 68\% of real-world container images are vulnerable from different image registries.},
  issue = {1},
  langid = {english},
  file = {/home/mechjm/Zotero/storage/DQ7886XL/Doan_Jung_2022_DAVS.pdf}
}

@online{DockerBuildx2022,
  title = {Docker Buildx},
  date = {2022-12-29T15:07:23+00:00},
  url = {https://docs.docker.com/engine/reference/commandline/buildx/},
  urldate = {2022-12-29},
  abstract = {docker buildx: Extended build capabilities with BuildKit},
  langid = {english},
  organization = {{Docker Documentation}},
  file = {/home/mechjm/Zotero/storage/DILUM4WF/buildx.html}
}

@inproceedings{duaPerformanceAnalysisUnion2016,
  title = {Performance Analysis of {{Union}} and {{CoW File Systems}} with {{Docker}}},
  booktitle = {2016 {{International Conference}} on {{Computing}}, {{Analytics}} and {{Security Trends}} ({{CAST}})},
  author = {Dua, Rajdeep and Kohli, Vaibhav and Patil, Sriram and Patil, Swapnil},
  date = {2016-12},
  pages = {550--555},
  doi = {10.1109/CAST.2016.7915029},
  abstract = {One of the biggest challenges that we face today in file systems is balancing performance, durability and consistency. With data growing beyond what was anticipated a decade ago, we need solutions and better file system. The new applications, platforms and systems currently built depend heavily on the file system for performance. One of such new platform is Docker, which is an open source project that enables Linux applications to be packaged as containers without typical overheads of virtual machines. This paper gives performance analysis and comparison of a file systems called as Unioning File Systems and Copy-onwrite File Systems. These file systems are used to provide core features of Docker storage. Docker follows a pluggable architecture which allows user to select the storage driver according to their need. This paper will give a clear picture and help the users in selecting the Docker storage driver according to the use case they are planning to implement in production.},
  eventtitle = {2016 {{International Conference}} on {{Computing}}, {{Analytics}} and {{Security Trends}} ({{CAST}})},
  keywords = {Benchmark testing,Containers,Copy-on-write File Systems,Cows,Docker,File systems,Linux,Linux Containers,Metadata,Performance evaluation,Unioning File Systems},
  file = {/home/mechjm/Zotero/storage/HR4FKQHZ/Dua et al_2016_Performance analysis of Union and CoW File Systems with Docker.pdf;/home/mechjm/Zotero/storage/U4JYKBRF/7915029.html}
}

@inproceedings{engRevisitingDockerfilesOpen2021,
  title = {Revisiting {{Dockerfiles}} in {{Open Source Software Over Time}}},
  booktitle = {2021 {{IEEE}}/{{ACM}} 18th {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  author = {Eng, Kalvin and Hindle, Abram},
  date = {2021-05},
  pages = {449--459},
  issn = {2574-3864},
  doi = {10.1109/MSR52588.2021.00057},
  abstract = {Docker is becoming ubiquitous with containerization for developing and deploying applications. Previous studies have analyzed Dockerfiles that are used to create container images in order to better understand how to improve Docker tooling. These studies obtain Dockerfiles using either Docker Hub or Github. In this paper, we revisit the findings of previous studies using the largest set of Dockerfiles known to date with over 9.4 million unique Dockerfiles found in the World of Code infrastructure spanning from 2013-2020. We contribute a historical view of the Dockerfile format by analyzing the Docker engine changelogs and use the history to enhance our analysis of Dockerfiles. We also reconfirm previous findings of a downward trend in using OS images and an upward trend of using language images. As well, we reconfirm that Dockerfile smell counts are slightly decreasing meaning that Dockerfile authors are likely getting better at following best practices. Based on these findings, it indicates that previous analyses from prior works have been correct in many of their findings and their suggestions to build better tools for Docker image creation are further substantiated.},
  eventtitle = {2021 {{IEEE}}/{{ACM}} 18th {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  keywords = {Cognition,Containers,Data mining,Docker,Education,Git,GitHub,History,Market research,Tools},
  file = {/home/mechjm/Zotero/storage/9ALV293B/Eng e Hindle - 2021 - Revisiting Dockerfiles in Open Source Software Ove.pdf;/home/mechjm/Zotero/storage/FMR5J3CE/9463072.html}
}

@online{GossExtrasDgoss,
  title = {Goss/Extras/Dgoss at Master · Goss-Org/Goss},
  url = {https://github.com/goss-org/goss},
  urldate = {2022-12-30},
  abstract = {Quick and Easy server testing/validation. Contribute to goss-org/goss development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/mechjm/Zotero/storage/7MXNJAUG/dgoss.html}
}

@article{harterSlackerFastDistribution,
  title = {Slacker: {{Fast Distribution}} with {{Lazy Docker Containers}}},
  author = {Harter, Tyler and Salmon, Brandon and Liu, Rose and Arpaci-Dusseau, Andrea C and Arpaci-Dusseau, Remzi H},
  abstract = {Containerized applications are becoming increasingly popular, but unfortunately, current containerdeployment methods are very slow. We develop a new container benchmark, HelloBench, to evaluate the startup times of 57 different containerized applications. We use HelloBench to analyze workloads in detail, studying the block I/O patterns exhibited during startup and compressibility of container images. Our analysis shows that pulling packages accounts for 76\% of container start time, but only 6.4\% of that data is read. We use this and other ndings to guide the design of Slacker, a new Docker storage driver optimized for fast container startup. Slacker is based on centralized storage that is shared between all Docker workers and registries. Workers quickly provision container storage using backend clones and minimize startup latency by lazily fetching container data. Slacker speeds up the median container development cycle by 20× and deployment cycle by 5×.},
  langid = {english},
  file = {/home/mechjm/Zotero/storage/M64S3BAD/Harter et al. - Slacker Fast Distribution with Lazy Docker Contai.pdf}
}

@inproceedings{hassanRUDSEARecommendingUpdates2018,
  title = {{{RUDSEA}}: Recommending Updates of {{Dockerfiles}} via Software Environment Analysis},
  shorttitle = {{{RUDSEA}}},
  booktitle = {Proceedings of the 33rd {{ACM}}/{{IEEE International Conference}} on {{Automated Software Engineering}}},
  author = {Hassan, Foyzul and Rodriguez, Rodney and Wang, Xiaoyin},
  date = {2018-09-03},
  series = {{{ASE}} 2018},
  pages = {796--801},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3238147.3240470},
  url = {https://doi.org/10.1145/3238147.3240470},
  urldate = {2022-12-01},
  abstract = {Dockerfiles are configuration files of docker images which package all dependencies of a software to enable convenient software deployment and porting. In other words, dockerfiles list all environment assumptions of a software application's build and / or execution, so they need to be frequently updated when the environment assumptions change during fast software evolution. In this paper, we propose RUDSEA, a novel approach to recommend updates of dockerfiles to developers based on analyzing changes on software environment assumptions and their impacts. Our evaluation on 1,199 real-world instruction updates shows that RUDSEA can recommend correct update locations for 78.5\% of the updates, and correct code changes for 44.1\% of the updates.},
  isbn = {978-1-4503-5937-5},
  keywords = {Dockerfile,Software Environment,String Analysis},
  file = {/home/mechjm/Zotero/storage/5HZNDD9G/Hassan et al. - 2018 - RUDSEA recommending updates of Dockerfiles via so.pdf}
}

@inproceedings{henkelDatasetDockerfiles2020,
  title = {A {{Dataset}} of {{Dockerfiles}}},
  booktitle = {Proceedings of the 17th {{International Conference}} on {{Mining Software Repositories}}},
  author = {Henkel, Jordan and Bird, Christian and Lahiri, Shuvendu K. and Reps, Thomas},
  date = {2020-09-18},
  series = {{{MSR}} '20},
  pages = {528--532},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3379597.3387498},
  url = {https://doi.org/10.1145/3379597.3387498},
  urldate = {2022-11-18},
  abstract = {Dockerfiles are one of the most prevalent kinds of DevOps artifacts used in industry. Despite their prevalence, there is a lack of sophisticated semantics-aware static analysis of Dockerfiles. In this paper, we introduce a dataset of approximately 178,000 unique Dockerfiles collected from GitHub. To enhance the usability of this data, we describe five representations we have devised for working with, mining from, and analyzing these Dockerfiles. Each Dockerfile representation builds upon the previous ones, and the final representation, created by three levels of nested parsing and abstraction, makes tasks such as mining and static checking tractable. The Dockerfiles, in each of the five representations, along with metadata and the tools used to shepard the data from one representation to the next are all available at: https://doi.org/10.5281/zenodo.3628771.},
  isbn = {978-1-4503-7517-7},
  keywords = {Bash; Mining,Datasets,DevOps,Docker},
  file = {/home/mechjm/Zotero/storage/C7V6MEHW/Henkel et al_2020_A Dataset of Dockerfiles.pdf}
}

@inproceedings{henkelLearningUnderstandingSupporting2020,
  title = {Learning from, {{Understanding}}, and {{Supporting DevOps Artifacts}} for {{Docker}}},
  booktitle = {2020 {{IEEE}}/{{ACM}} 42nd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Henkel, Jordan and Bird, Christian and Lahiri, Shuvendu K. and Reps, Thomas},
  date = {2020-10},
  pages = {38--49},
  issn = {1558-1225},
  doi = {10.1145/3377811.3380406},
  abstract = {With the growing use of DevOps tools and frameworks, there is an increased need for tools and techniques that support more than code. The current state-of-the-art in static developer assistance for tools like Docker is limited to shallow syntactic validation. We identify three core challenges in the realm of learning from, understanding, and supporting developers writing DevOps artifacts: (i) nested languages in DevOps artifacts, (ii) rule mining, and (iii) the lack of semantic rule-based analysis. To address these challenges we introduce a toolset, binnacle, that enabled us to ingest 900,000 GitHub repositories. Focusing on Docker, we extracted approximately 178,000 unique Dockerfiles, and also identified a Gold Set of Dockerfiles written by Docker experts. We addressed challenge (i) by reducing the number of effectively uninterpretable nodes in our ASTs by over 80\% via a technique we call phased parsing. To address challenge (ii), we introduced a novel rule-mining technique capable of recovering two-thirds of the rules in a benchmark we curated. Through this automated mining, we were able to recover 16 new rules that were not found during manual rule collection. To address challenge (iii), we manually collected a set of rules for Dockerfiles from commits to the files in the Gold Set. These rules encapsulate best practices, avoid docker build failures, and improve image size and build latency. We created an analyzer that used these rules, and found that, on average, Dockerfiles on GitHub violated the rules five times more frequently than the Dockerfiles in our Gold Set. We also found that industrial Dockerfiles fared no better than those sourced from GitHub. The learned rules and analyzer in binnacle can be used to aid developers in the IDE when creating Dockerfiles, and in a post-hoc fashion to identify issues in, and to improve, existing Dockerfiles.},
  eventtitle = {2020 {{IEEE}}/{{ACM}} 42nd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  keywords = {Data mining,DevOps,Docker,Ecosystems,Encoding,Gold,Maintenance engineering,Mining,Software development management,Static Checking,Tools},
  file = {/home/mechjm/Zotero/storage/9DSHAB5A/Henkel et al_2020_Learning from, Understanding, and Supporting DevOps Artifacts for Docker.pdf;/home/mechjm/Zotero/storage/P8TQFWVC/9284129.html}
}

@inproceedings{henkelShipwrightHumanintheLoopSystem2021,
  title = {Shipwright: {{A Human-in-the-Loop System}} for {{Dockerfile Repair}}},
  shorttitle = {Shipwright},
  booktitle = {2021 {{IEEE}}/{{ACM}} 43rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Henkel, Jordan and Silva, Denini and Teixeira, Leopoldo and d’ Amorim, Marcelo and Reps, Thomas},
  options = {useprefix=true},
  date = {2021-05},
  pages = {1148--1160},
  issn = {1558-1225},
  doi = {10.1109/ICSE43902.2021.00106},
  abstract = {Docker is a tool for lightweight OS-level virtualization. Docker images are created by performing a build, controlled by a source-level artifact called a Dockerfile. We studied Dockerfiles on GitHub, and-to our great surprise-found that over a quarter of the examined Dockerfiles failed to build (and thus to produce images). To address this problem, we propose SHIPWRIGHT, a human-in-the-loop system for finding repairs to broken Dockerfiles. SHIPWRIGHT uses a modified version of the BERT language model to embed build logs and to cluster broken Dockerfiles. Using these clusters and a search-based procedure, we were able to design 13 rules for making automated repairs to Dockerfiles. With the aid of SHIPWRIGHT, we submitted 45 pull requests (with a 42.2\% acceptance rate) to GitHub projects with broken Dockerfiles. Furthermore, in a "time-travel" analysis of broken Dockerfiles that were later fixed, we found that SHIPWRIGHT proposed repairs that were equivalent to human-authored patches in 22.77\% of the cases we studied. Finally, we compared our work with recent, state-of-the-art, static Dockerfile analyses, and found that, while static tools detected possible build-failure-inducing issues in 20.6-33.8\% of the files we examined, SHIPWRIGHT was able to detect possible issues in 73.25\% of the files and, additionally, provide automated repairs for 18.9\% of the files.},
  eventtitle = {2021 {{IEEE}}/{{ACM}} 43rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  keywords = {Bit error rate,DevOps,Docker,Maintenance engineering,Repair,Software development management,Software engineering,Tools,Virtualization},
  file = {/home/mechjm/Zotero/storage/J8GW4ADJ/Henkel et al_2021_Shipwright.pdf;/home/mechjm/Zotero/storage/W3DRMDCN/9402069.html}
}

@inproceedings{hortonDockerizeMeAutomaticInference2019,
  title = {{{DockerizeMe}}: {{Automatic Inference}} of {{Environment Dependencies}} for {{Python Code Snippets}}},
  shorttitle = {{{DockerizeMe}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Horton, Eric and Parnin, Chris},
  date = {2019-05},
  pages = {328--338},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2019.00047},
  abstract = {Platforms like Stack Overflow and GitHub's gist system promote the sharing of ideas and programming techniques via the distribution of code snippets designed to illustrate particular tasks. Python, a popular and fast-growing programming language, sees heavy use on both sites, with nearly one million questions asked on Stack Overflow and 400 thousand public gists on GitHub. Unfortunately, around 75\% of the Python example code shared through these sites cannot be directly executed. When run in a clean environment, over 50\% of public Python gists fail due to an import error for a missing library. We present DockerizeMe, a technique for inferring the dependencies needed to execute a Python code snippet without import error. DockerizeMe starts with offline knowledge acquisition of the resources and dependencies for popular Python packages from the Python Package Index (PyPI). It then builds Docker specifications using a graph-based inference procedure. Our inference procedure resolves import errors in 892 out of nearly 3,000 gists from the Gistable dataset for which Gistable's baseline approach could not find and install all dependencies.},
  eventtitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  keywords = {Configuration Management,Dependencies,Docker,Environment Inference,Indexes,Inference algorithms,Knowledge based systems,Libraries,Python,Task analysis,Wheels},
  file = {/home/mechjm/Zotero/storage/CP99H9PD/Horton_Parnin_2019_DockerizeMe.pdf;/home/mechjm/Zotero/storage/B5P8G6IA/8811897.html}
}

@inproceedings{huangFastBuildAcceleratingDocker2019,
  title = {{{FastBuild}}: {{Accelerating Docker Image Building}} for {{Efficient Development}} and {{Deployment}} of {{Container}}},
  shorttitle = {{{FastBuild}}},
  booktitle = {2019 35th {{Symposium}} on {{Mass Storage Systems}} and {{Technologies}} ({{MSST}})},
  author = {Huang, Zhuo and Wu, Song and Jiang, Song and Jin, Hai},
  date = {2019-05},
  pages = {28--37},
  issn = {2160-1968},
  doi = {10.1109/MSST.2019.00-18},
  abstract = {Docker containers have been increasingly adopted on various computing platforms to provide a lightweight virtualized execution environment. Compared to virtual machines, this technology can often reduce the launch time from a few minutes to less than 10 seconds, assuming the Docker image has been locally available. However, Docker images are highly customizable, and are mostly built at runtime from a remote base image by running instructions in a script (the Dockerfile). During the instruction execution, a large number of input files may have to be retrieved via the Internet. The image building may be an iterative process as one may need to repeatedly modify the Dockerfile until a desired image composition is received. In the process, every input file required by an instruction has to be remotely retrieved, even if it has been recently downloaded. This can make the process of building of an image and launching of a container unexpectedly slow. To address the issue, we propose a technique, named FastBuild, that maintains a local file cache to minimize the expensive file downloading. By non-intrusively intercepting remote file requests, and supplying files locally, FastBuild enables file caching in a manner transparent to image building. To further accelerate the image building, FastBuild overlaps operations of instructions' execution and writing intermediate image layers to the disk. We have implemented FastBuild. And experiments with images and Dockerfiles obtained from Docker Hub show that the system can improve building speed by up to 10 times, and reduce downloaded data by 72\%.},
  eventtitle = {2019 35th {{Symposium}} on {{Mass Storage Systems}} and {{Technologies}} ({{MSST}})},
  keywords = {Acceleration,Buildings,Computer science,container,Containers,Docker,image building,Internet,Servers,Virtual machining},
  file = {/home/mechjm/Zotero/storage/3XNI33CY/Huang et al_2019_FastBuild.pdf;/home/mechjm/Zotero/storage/P8EMEM7J/8890109.html}
}

@article{jainStaticVulnerabilityAnalysis2021,
  title = {Static {{Vulnerability Analysis}} of {{Docker Images}}},
  author = {Jain, Vipin and Singh, Baldev and Khenwar, Medha and Sharma, Milind},
  date = {2021-04},
  journaltitle = {IOP Conference Series: Materials Science and Engineering},
  shortjournal = {IOP Conf. Ser.: Mater. Sci. Eng.},
  volume = {1131},
  number = {1},
  pages = {012018},
  publisher = {{IOP Publishing}},
  issn = {1757-899X},
  doi = {10.1088/1757-899X/1131/1/012018},
  url = {https://dx.doi.org/10.1088/1757-899X/1131/1/012018},
  urldate = {2022-12-15},
  abstract = {Many organizations are renovating their businesses by grasping DevOps, microservices, and container technologies. Docker is emerged as a new technology, proving an efficient means to develop and deploy applications. Docker containers are created by images to run an application with all its dependencies so that it could run isolated from other processes. Security is always being a foremost concern as our industries are already persistent to improve the reliability and efficiency of new software applications. Security of local Docker containers from the attacks of malicious containers, perceived threats present in Docker images need to be detected and the risks identified when instances of Docker containers run on the host machine. This paper reviews Docker’s existing security mechanisms, vulnerabilities, threats and the related tools required for static security analysis.},
  langid = {english},
  file = {/home/mechjm/Zotero/storage/JJFG2H48/Jain et al_2021_Static Vulnerability Analysis of Docker Images.pdf}
}

@inproceedings{kehrerContainerBasedModuleIsolation2019,
  title = {Container-{{Based Module Isolation}} for {{Cloud Services}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Service-Oriented System Engineering}} ({{SOSE}})},
  author = {Kehrer, Stefan and Riebandt, Florian and Blochinger, Wolfgang},
  date = {2019-04},
  pages = {177--17709},
  issn = {2642-6587},
  doi = {10.1109/SOSE.2019.00032},
  abstract = {Due to frequently changing requirements, the internal structure of cloud services is highly dynamic. To ensure flexibility, adaptability, and maintainability for dynamically evolving services, modular software development has become the dominating paradigm. By following this approach, services can be rapidly constructed by composing existing, newly developed and publicly available third-party modules. However, newly added modules might be unstable, resource-intensive, or untrustworthy. Thus, satisfying non-functional requirements such as reliability, efficiency, and security while ensuring rapid release cycles is a challenging task. In this paper, we discuss how to tackle these issues by employing container virtualization to isolate modules from each other according to a specification of isolation constraints. We satisfy non-functional requirements for cloud services by automatically transforming the modules comprised into a container-based system. To deal with the increased overhead that is caused by isolating modules from each other, we calculate the minimum set of containers required to satisfy the isolation constraints specified. Moreover, we present and report on a prototypical transformation pipeline that automatically transforms cloud services developed based on the Java Platform Module System into container-based systems.},
  eventtitle = {2019 {{IEEE International Conference}} on {{Service-Oriented System Engineering}} ({{SOSE}})},
  keywords = {Cloud computing,container virtualization,Containers,continuous delivery,deployment automation,DevOps,Monitoring,non-functional requirements,Pipelines,Security,Software,Virtualization},
  file = {/home/mechjm/Zotero/storage/3Q6A3Y9Z/Kehrer et al_2019_Container-Based Module Isolation for Cloud Services.pdf;/home/mechjm/Zotero/storage/SLVDT8X2/8705914.html}
}

@inproceedings{kitajimaLatestImageRecommendation2020,
  title = {Latest {{Image Recommendation Method}} for {{Automatic Base Image Update}} in~{{Dockerfile}}},
  booktitle = {Service-{{Oriented Computing}}},
  author = {Kitajima, Shinya and Sekiguchi, Atsuji},
  editor = {Kafeza, Eleanna and Benatallah, Boualem and Martinelli, Fabio and Hacid, Hakim and Bouguettaya, Athman and Motahari, Hamid},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {547--562},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-65310-1_40},
  abstract = {In recent years, an application deployment method using Docker container has attracted attention by researchers. Docker containers are fast and lightweight, can improve the portability and reproducibility of applications, and are thus often used with CI/CD and DevOps to accelerate the release cycle. However, if a Docker image is not updated, problems such as security risks or a lack of the latest features may occur. Therefore, in this paper, we propose a method for automatically updating the base image to the latest version when the image is considered to be the old version. Our method extracts the information of the base image from the Dockerfile described by the user, and infers the version of the base image that is considered to be certainly used. By applying our method, the user can regularly update the base image. Based on the evaluation result, we confirmed that our method recommends an approximately correct version to the users.},
  isbn = {978-3-030-65310-1},
  langid = {english},
  keywords = {Automatic update,Dockerfile,PaaS,Semantic versioning},
  file = {/home/mechjm/Zotero/storage/I75RQF24/Kitajima_Sekiguchi_2020_Latest Image Recommendation Method for Automatic Base Image Update in Dockerfile.pdf}
}

@article{ksontiniRefactoringsTechnicalDebt,
  title = {Refactorings and {{Technical Debt}} for {{Docker Projects}}},
  author = {Ksontini, Emna and Kessentini, Marouane},
  abstract = {Software containers, such as Docker, are recently considered as the mainstream technology of providing reusable software artifacts. Developers can easily build and deploy their applications based on the large number of reusable Docker images that are publicly available. Thus, a current popular trend in industry is to move towards the containerization of their applications. However, container-based projects compromise different components including the Docker and Docker-compose files, and several other dependencies to the source code combining different containers and facilitating the interactions with them. Similar to any other complex systems, container-based projects are prone to various quality and technical debt issues related to different artifacts: Docker and Docker-compose files, and regular source code ones. Unfortunately, there is a gap of knowledge in how container-based projects actually evolve and are maintained.},
  langid = {english},
  file = {/home/mechjm/Zotero/storage/VYVRIPZU/Ksontini e Kessentini - Refactorings and Technical Debt for Docker Project.pdf}
}

@misc{liDockerMockPreBuildDetection2021,
  title = {{{DockerMock}}: {{Pre-Build Detection}} of {{Dockerfile Faults}} through {{Mocking Instruction Execution}}},
  shorttitle = {{{DockerMock}}},
  author = {Li, Mingjie and Bai, Xiaoying and Ma, Minghua and Pei, Dan},
  date = {2021-04-12},
  number = {arXiv:2104.05490},
  eprint = {2104.05490},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.05490},
  url = {http://arxiv.org/abs/2104.05490},
  urldate = {2022-11-22},
  abstract = {Continuous Integration (CI) and Continuous Deployment (CD) are widely adopted in software engineering practice. In reality, the CI/CD pipeline execution is not yet reliably continuous because it is often interrupted by Docker build failures. However, the existing trial-and-error practice to detect faults is time-consuming. To timely detect Dockerfile faults, we propose a context-based pre-build analysis approach, named DockerMock, through mocking the execution of common Dockerfile instructions. A Dockerfile fault is declared when an instruction conflicts with the approximated and accumulated running context. By explicitly keeping track of whether the context is fuzzy, DockerMock strikes a good balance of detection precision and recall. We evaluated DockerMock with 53 faults in 41 Dockerfiles from open source projects on GitHub and 130 faults in 105 Dockerfiles from student course projects. On average, DockerMock detected 68.0\% Dockerfile faults in these two datasets. While baseline hadolint detected 6.5\%, and baseline BuildKit detected 60.5\% without instruction execution. In the GitHub dataset, DockerMock reduces the number of builds to 47, outperforming that of hadolint (73) and BuildKit (74).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering},
  file = {/home/mechjm/Zotero/storage/E86I8S7A/Li et al_2021_DockerMock.pdf;/home/mechjm/Zotero/storage/VL86LDYD/2104.html}
}

@article{linnalampiOutdatedSoftwareContainer2021,
  title = {Outdated Software in Container Images},
  author = {Linnalampi, Markus},
  date = {2021-08-23},
  url = {https://aaltodoc.aalto.fi:443/handle/123456789/109398},
  urldate = {2022-11-20},
  abstract = {In recent years, containers have become a central part of modern software development. Containers enable faster development cycle, better reproducibility and are ideal for modern microservice architectures. Docker has become the de facto standard of containers, but recently OCI has made most of the containers compatible and provider independent.      As containers have become more essential, their security has become a major concern. Containers have significantly different technical implementation than the old virtualized servers. Because of this, the old best practices are not always relevant anymore with the new technology. Containers have a completely new security environment, which offers both new ways of improving the security of software, but also creates new problems. This is why understanding the container security has become essential.      First, this thesis discusses the history of containers, the reasons why containers have become popular in recent years as well as the technical background of containers. After that, the most essential best practices for container security and the reasoning behind them is discussed. The actual research topic of this thesis is solutions to detecting outdated software versions from container images. The thesis discusses the most popular existing tools for this and shows results for most popular container images. After that, the limitations of existing tools is discussed, a new approach for the problem is proposed and the new method and existing tools are compared.},
  langid = {english},
  annotation = {Accepted: 2021-08-29T17:18:45Z},
  file = {/home/mechjm/Zotero/storage/P6GHRSUE/Linnalampi_2021_Outdated software in container images.pdf;/home/mechjm/Zotero/storage/PECPQ9SE/109398.html}
}

@article{luEmpiricalCaseStudy2019,
  title = {An {{Empirical Case Study}} on the {{Temporary File Smell}} in {{Dockerfiles}}},
  author = {Lu, Zhigang and Xu, Jiwei and Wu, Yuewen and Wang, Tao and Huang, Tao},
  date = {2019},
  journaltitle = {IEEE Access},
  volume = {7},
  pages = {63650--63659},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2905424},
  abstract = {Docker is widely used in data centers to host services. The docker image adopts a hierarchical storage architecture, which means that the docker image is composed of a set of filesystem layers. In the image building process, only the top layer is read-write, while the bottom layers are all read-only. However, temporary files are often used in the image building process. Nevertheless, if a temporary file is imported and removed in different layers by a careless developer, it will lead a file redundancy. We termed this problem “temporary file smell.” This smell leads to larger-size images, which seriously restricts the efficiency of image distribution and thus affects the scaling ability of services in facing of sudden high load. To address this problem, we make an empirical case study to the real-world Dockerfiles on DockerHub. Based on the case study, we summarize four different smell patterns and propose a state-depend static analysis method to detect this kind of smells. We also provide three feasible fixing methods as selective options to eliminate the temporary file smell.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Abstract syntax tree,Best practices,Buildings,Containers,Docker,Dockerfile,Kernel,static analysis,Static analysis,Syntactics,temporary file smell,Tools},
  file = {/home/mechjm/Zotero/storage/ZTKKEFLL/Lu et al_2019_An Empirical Case Study on the Temporary File Smell in Dockerfiles.pdf;/home/mechjm/Zotero/storage/U2SHQHQ5/8667832.html}
}

@article{maduroAutomaticServiceContainerization2021,
  title = {Automatic {{Service Containerization}} with {{Docker}}},
  author = {Maduro, João Carlos Cardoso},
  date = {2021-07-19},
  url = {https://repositorio-aberto.up.pt/handle/10216/135486},
  urldate = {2022-11-17},
  langid = {english},
  annotation = {Accepted: 2022-09-07T17:51:15Z},
  file = {/home/mechjm/Zotero/storage/5KBGRUIW/Maduro_2021_Automatic Service Containerization with Docker.pdf;/home/mechjm/Zotero/storage/LFW4JMAP/135486.html}
}

@inproceedings{maloneyDirectnessLivenessMorphic1995,
  title = {Directness and Liveness in the Morphic User Interface Construction Environment},
  booktitle = {Proceedings of the 8th Annual {{ACM}} Symposium on {{User}} Interface and Software Technology  - {{UIST}} '95},
  author = {Maloney, John H. and Smith, Randall B.},
  date = {1995},
  pages = {21--28},
  publisher = {{ACM Press}},
  location = {{Pittsburgh, Pennsylvania, United States}},
  doi = {10.1145/215585.215636},
  url = {http://portal.acm.org/citation.cfm?doid=215585.215636},
  urldate = {2022-11-22},
  eventtitle = {The 8th Annual {{ACM}} Symposium},
  isbn = {978-0-89791-709-4},
  langid = {english},
  file = {/home/mechjm/Zotero/storage/3BQD9F4N/Maloney e Smith - 1995 - Directness and liveness in the morphic user interf.pdf}
}

@article{mcardleContainerImageOptimisation2016,
  title = {Container {{Image Optimisation}} and {{Security Practices}}},
  shorttitle = {{{THE DUBLIN DASHBOARD}}},
  author = {McArdle, G. and Kitchin, R.},
  date = {2016-09-05},
  journaltitle = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  shortjournal = {ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci.},
  volume = {IV-4/W1},
  pages = {19--25},
  issn = {2194-9050},
  doi = {10.5194/isprs-annals-IV-4-W1-19-2016},
  url = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-4-W1/19/2016/},
  urldate = {2022-10-27},
  abstract = {Cloud Computing is one of the major developments that offers a promising future. One of the basic tools which makes the Cloud a reality is virtualization. Hypervisor-based virtualization comes with high performance and overhead performance due to the added level of abstraction. Virtual Machine results in high service downtime whenever the application is updated to the new version. Containers have various advantages over virtual machines due to performance enhancements and reduced start time. Docker is widely used in the business environment and at the personal level container environment. If the Docker image has been stored locally on the server, Docker technology can frequently reduce the launch time from a few minutes to less than 5 seconds. However, these container images are very customisable and are typically created at runtime by executing script instructions from a remote base image (the Docker file). Many input files, including the packages and dependencies, may need to be acquired from the Internet during the execution of the instruction. The process of creating an image can be time-consuming and iterative.},
  langid = {english},
  file = {/home/mechjm/Zotero/storage/2RYN6ELE/McArdle e Kitchin - 2016 - THE DUBLIN DASHBOARD DESIGN AND DEVELOPMENT OF A .pdf}
}

@article{mcmillanMAKINGCONTAINERSEASIER,
  title = {{{MAKING CONTAINERS EASIER WITH HPC CONTAINER MAKER}}},
  author = {McMillan, Scott},
  pages = {47},
  langid = {english},
  file = {/home/mechjm/Zotero/storage/8AVZTJ7C/McMillan - MAKING CONTAINERS EASIER WITH HPC CONTAINER MAKER.pdf}
}

@article{moralesProgrammerEXperienceSystematic2019,
  title = {Programmer {{eXperience}}: {{A Systematic Literature Review}}},
  shorttitle = {Programmer {{eXperience}}},
  author = {Morales, Jenny and Rusu, Cristian and Botella, Federico and Quiñones, Daniela},
  date = {2019},
  journaltitle = {IEEE Access},
  volume = {7},
  pages = {71079--71094},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2920124},
  abstract = {Programmers use various software development artifacts in their work, such as programming environments, design documents, and programming codes. These software artifacts can be studied and improved based on usability and User eXperience (UX) factors. In this paper, we consider programmers to be a specific case of users and analyze different elements that influence their experience in this specific context. We conducted a systematic literature review of papers published over the last ten years related to 1) the definition of the Programmer eXperience (PX); 2) the PX, UX, and usability factors regarding the programming environments, design documents, and programming codes; and 3) sets of heuristics to evaluate the software development artifacts mentioned before. We analyzed 73 articles, and the results obtained show that: 1) the important elements that influence the PX are the motivation of programmers and the choice of tools they use in their work, such as programming environments; 2) most of the identified studies (59\%) aimed to evaluate the influence of the PX, UX, and usability on programming environments; 3) the majority of the studies (70\%) used methods such as usability tests and/or heuristic evaluation methods; and 4) four sets of heuristics are used to evaluate software development artifacts in relation to programming environments, programming languages, and application programming interfaces. The results suggest that further research in this area is necessary to better understand and evaluate the concept of the PX.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Bibliographies,Heuristic evaluation,Programmer eXperience,Programming,Programming environments,systematic literature review,Systematics,usability,Usability,User experience,User eXperience},
  file = {/home/mechjm/Zotero/storage/Z3B4JII3/Morales et al_2019_Programmer eXperience.pdf;/home/mechjm/Zotero/storage/26MYANT2/8727527.html}
}

@article{nustTenSimpleRules2020,
  title = {Ten Simple Rules for Writing {{Dockerfiles}} for Reproducible Data Science},
  author = {Nüst, Daniel and Sochat, Vanessa and Marwick, Ben and Eglen, Stephen J. and Head, Tim and Hirst, Tony and Evans, Benjamin D.},
  date = {2020-11-10},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {16},
  number = {11},
  pages = {e1008316},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008316},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008316},
  urldate = {2022-12-16},
  abstract = {Computational science has been greatly improved by the use of containers for packaging software and data dependencies. In a scholarly context, the main drivers for using these containers are transparency and support of reproducibility; in turn, a workflow’s reproducibility can be greatly affected by the choices that are made with respect to building containers. In many cases, the build process for the container’s image is created from instructions provided in a Dockerfile format. In support of this approach, we present a set of rules to help researchers write understandable Dockerfiles for typical data science workflows. By following the rules in this article, researchers can create containers suitable for sharing with fellow scientists, for including in scholarly communication such as education or scientific papers, and for effective and sustainable personal workflows.},
  langid = {english},
  keywords = {Computer and information sciences,Computer software,Habits,Metadata,Programming languages,Reproducibility,Software tools,Source code},
  file = {/home/mechjm/Zotero/storage/Y5V6XLHF/Nüst et al_2020_Ten simple rules for writing Dockerfiles for reproducible data science.pdf}
}

@article{osorioDockerPediaKnowledgeGraph,
  title = {{{DockerPedia}}: A {{Knowledge Graph}} of {{Docker Images}}},
  author = {Osorio, Maximiliano and Buil-Aranda, Carlos and Vargas, Hernan},
  abstract = {Docker is the most popular implementation of Operating System virtualization, currently its online registry service (Docker Hub) stores more than 4.5 millions of software images. Using that registry it is possible to download and deploy Docker images as software containers. However, these images only show information of the main software, hiding the dependencies needed to run it. To allow users to track what they deploy into their machines, we developed DockerPedia, a resource that publishes information of the packages within the Docker images as Linked Data. Currently our resource includes 28\% of the most downloaded images from Docker Hub providing information about the software dependencies and its vulnerabilities allowing to easily reproduce the environment in which each image was deployed as well as to check the security of the image without the need to download it.},
  langid = {english},
  file = {/home/mechjm/Zotero/storage/YI46R6QM/Osorio et al. - DockerPedia a Knowledge Graph of Docker Images.pdf}
}

@article{prinettoSecurityMisconfigurationsDetection,
  title = {Security {{Misconfigurations Detection}} and {{Repair}} in {{Dockerfile}}},
  author = {Prinetto, Paolo Ernesto and Bortolameotti, Dott Riccardo and Massaro, Giuseppe},
  pages = {78},
  langid = {english},
  file = {/home/mechjm/Zotero/storage/YUGFDRWA/Prinetto et al. - Security Misconfigurations Detection and Repair in.pdf}
}

@inproceedings{rastogiCimplifierAutomaticallyDebloating2017,
  title = {Cimplifier: Automatically Debloating Containers},
  shorttitle = {Cimplifier},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Rastogi, Vaibhav and Davidson, Drew and De Carli, Lorenzo and Jha, Somesh and McDaniel, Patrick},
  date = {2017-08-21},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {476--486},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3106237.3106271},
  url = {https://doi.org/10.1145/3106237.3106271},
  urldate = {2022-12-22},
  abstract = {Application containers, such as those provided by Docker, have recently gained popularity as a solution for agile and seamless software deployment. These light-weight virtualization environments run applications that are packed together with their resources and configuration information, and thus can be deployed across various software platforms. Unfortunately, the ease with which containers can be created is oftentimes a double-edged sword, encouraging the packaging of logically distinct applications, and the inclusion of significant amount of unnecessary components, within a single container. These practices needlessly increase the container size-sometimes by orders of magnitude. They also decrease the overall security, as each included component-necessary or not-may bring in security issues of its own, and there is no isolation between multiple applications packaged within the same container image. We propose algorithms and a tool called Cimplifier, which address these concerns: given a container and simple user-defined constraints, our tool partitions it into simpler containers, which (i) are isolated from each other, only communicating as necessary, and (ii) only include enough resources to perform their functionality. Our evaluation on real-world containers demonstrates that Cimplifier preserves the original functionality, leads to reduction in image size of up to 95\%, and processes even large containers in under thirty seconds.},
  isbn = {978-1-4503-5105-8},
  keywords = {containers,debloating,least privilege,privilege separation},
  file = {/home/mechjm/Zotero/storage/ZXYJ5LPL/Rastogi et al_2017_Cimplifier.pdf}
}

@inproceedings{rastogiNewDirectionsContainer2017,
  title = {New {{Directions}} for {{Container Debloating}}},
  booktitle = {Proceedings of the 2017 {{Workshop}} on {{Forming}} an {{Ecosystem Around Software Transformation}}},
  author = {Rastogi, Vaibhav and Niddodi, Chaitra and Mohan, Sibin and Jha, Somesh},
  date = {2017-11-03},
  series = {{{FEAST}} '17},
  pages = {51--56},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3141235.3141241},
  url = {https://doi.org/10.1145/3141235.3141241},
  urldate = {2022-12-20},
  abstract = {Application containers, such as Docker containers, are light-weight virtualization environments that "contain" applications together with their resources and configuration information. While they are becoming increasingly popular as a method for agile software deployment, current techniques for preparing containers add unnecessary bloat into them: they often include unneeded files that increase the container size by several orders of magnitude. This not only leads to storage and network transfer issues but also security concerns. The problem is well-recognized but available solutions are mostly ad-hoc and not largely deployed. Our previous work, Cimplifier, on debloating containers uses dynamic analysis to identify the resources necessary to a container and then debloat it. However, the dynamic analysis uses model executions or test runs, which if incomplete, may not allow detection of all the necessary resources. Therefore, it is important to explore other directions towards container debloating. In this paper, we discuss two of them: a new intermediate representation allowing incorporation of multiple techniques, such as dynamic analysis and static analysis, for debloating; and test case augmentation using symbolic execution.},
  isbn = {978-1-4503-5395-3},
  keywords = {containers,debloating,least privilege},
  file = {/home/mechjm/Zotero/storage/IEXFK7DD/Rastogi et al_2017_New Directions for Container Debloating.pdf}
}

@inproceedings{reisDockerliveLiveDevelopment2022,
  title = {Dockerlive : {{A}} Live Development Environment for {{Dockerfiles}}},
  shorttitle = {Dockerlive},
  booktitle = {2022 {{IEEE Symposium}} on {{Visual Languages}} and {{Human-Centric Computing}} ({{VL}}/{{HCC}})},
  author = {Reis, David and Correia, Filipe F.},
  date = {2022-09-12},
  pages = {1--4},
  publisher = {{IEEE}},
  location = {{Roma, Italy}},
  doi = {10.1109/VL/HCC53370.2022.9833145},
  url = {https://ieeexplore.ieee.org/document/9833145/},
  urldate = {2022-10-22},
  abstract = {The process of developing Dockerfiles is perceived by many developers as slow and based on trial-and-error, and it is hardly immediate to see the result of a change introduced into a Dockerfile. In this work we propose a plugin for Visual Studio Code, which we name Dockerlive, and that has the purpose of shortening the length of feedback loops. Namely, the plugin is capable of providing information to developers on a number of Dockerfile elements, as the developer is writing the Dockerfile. We achieve this through dynamic analysis of the resulting container, which the plugin builds and runs in the background.},
  eventtitle = {2022 {{IEEE Symposium}} on {{Visual Languages}} and {{Human-Centric Computing}} ({{VL}}/{{HCC}})},
  isbn = {978-1-66544-214-5},
  langid = {english},
  file = {/home/mechjm/Zotero/storage/FUMSBADG/Reis e Correia - 2022 - Dockerlive  A live development environment for Do.pdf}
}

@article{reisLiveDockerContainers2020,
  title = {Live {{Docker Containers}}},
  author = {Reis, David Alexandre Gomes},
  date = {2020-07-21},
  url = {https://repositorio-aberto.up.pt/handle/10216/128956},
  urldate = {2022-11-16},
  abstract = {The use of containerization technologies for software development, such as Docker, is now widespread, with over 70000 Dockerfiles being found in projects from the GitHub platform as of October 2016. The use of these technologies has a few advantages, providing a secure, portable and efficient environment where applications can be executed. Currently, the usual workflow of a developer configuring a Docker environment consists of writing a Dockerfile, building the Dockerfile into a Docker image, instantiating that Docker image in a Docker container and verifying if the container is working as expected (using a tool or the command-line). If the container is not behaving as expected, then the developer has to make changes to the Dockerfile and repeat the process, until the desired behaviour is achieved. This process is often slow, based on trial-and-error, and therefore time consuming and frustrating for developers, as observed in a survey performed with students with some Docker experience. As such, reducing the temporal distance between editing a Dockerfile and observing the consequences of those changes on the containers that can be built from that Dockerfile (i.e. tightening the feedback loop) can reduce the debugging efforts required by the programmer.  Live programming refers to the ability to obtain continuous feedback on a program while that program is being developed. Since software is usually developed on text editors/IDEs, live feedback is usually provided within these tools. The level of liveness in IDEs is related to the type and update frequency of the feedback provided. The existing tools that can help a developer working with Dockerfiles can be split into several categories: container status, performance monitoring, container management, infrastructure testing, static analysis and image build optimization. However, currently the only tools which provide live feedback are some static analysis tools. Therefore, the developer is restricted to the development workflow mentioned above, with little liveness in his environment. Providing more live dynamic feedback in the developer's environment is expected to lead to an increased efficiency of developers working on Dockerfiles. This can be achieved through the implementation of an IDE plugin which automatically builds, instantiates and extracts information from a Docker container while the developer writes the respective Dockerfile, providing continuous feedback on changes that the developer makes. In order to quantify the influence of providing more live dynamic feedback on the efficiency of developers, a user study was conducted.},
  langid = {english},
  annotation = {Accepted: 2022-09-16T05:32:27Z},
  file = {/home/mechjm/Zotero/storage/G7BD6WNM/Reis - 2020 - Live Docker Containers.pdf;/home/mechjm/Zotero/storage/JHC85ELK/128956.html}
}

@inproceedings{rosaAssessingImprovingQuality2022,
  title = {Assessing and {{Improving}} the {{Quality}} of {{Docker Artifacts}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  author = {Rosa, Giovanni and Scalabrino, Simone and Oliveto, Rocco},
  date = {2022-10},
  pages = {592--596},
  publisher = {{IEEE}},
  location = {{Limassol, Cyprus}},
  doi = {10.1109/ICSME55016.2022.00081},
  url = {https://ieeexplore.ieee.org/document/9978228/},
  urldate = {2022-12-26},
  abstract = {Docker is the most diffused containerization technology adopted in the DevOps workflow. Docker allows shipping applications in Docker images, along with their dependencies and execution environment. A Docker image is created using a configuration file called Dockerfile. The literature shows that quality issues, such as violations of best practices (i.e., Dockerfile smells), are diffused among Docker artifacts. Smells can negatively impact the reliability, leading to building failures, poor performance, and security issues. In addition, it is unclear to what extent developers are aware of those quality issues and what quality aspects are correlated with the adoption of a Docker image. As evaluated in the literature, composing high-quality Dockerfiles and Docker images is not a trivial task. In this research, we aim to propose approaches and techniques to assess and improve the quality of Dockerfiles and Docker images. First, starting from the resolution of Dockerfile smells, we aim to improve the internal and then the related external quality aspects that also affect the developers’ preference and the perceived quality when they adopt a Docker image. Next, we want to employ that knowledge in the automated generation of high-quality Dockerfiles and Docker images.},
  eventtitle = {2022 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  isbn = {978-1-66547-956-1},
  langid = {english},
  file = {/home/mechjm/Zotero/storage/QSQZZ78N/Rosa et al. - 2022 - Assessing and Improving the Quality of Docker Arti.pdf}
}

@misc{rosaFixingDockerfileSmells2022,
  title = {Fixing {{Dockerfile Smells}}: {{An Empirical Study}}},
  shorttitle = {Fixing {{Dockerfile Smells}}},
  author = {Rosa, Giovanni and Scalabrino, Simone and Oliveto, Rocco},
  date = {2022-08-18},
  number = {arXiv:2208.09097},
  eprint = {2208.09097},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2208.09097},
  urldate = {2022-10-26},
  abstract = {Background. Containerization technologies are widely adopted in the DevOps workflow. The most commonly used one is Docker, which requires developers to define a specification file (Dockerfile) to build the image used for creating containers. There are several best practice rules for writing Dockerfiles, but the developers do not always follow them. Violations of such practices, known as Dockerfile smells, can negatively impact the reliability and the performance of Docker images. Previous studies showed that Dockerfile smells are widely diffused, and there is a lack of automatic tools that support developers in fixing them. However, it is still unclear what Dockerfile smells get fixed by developers and to what extent developers would be willing to fix smells in the first place.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Software Engineering},
  file = {/home/mechjm/Zotero/storage/7FR8HPZ4/Rosa et al. - 2022 - Fixing Dockerfile Smells An Empirical Study.pdf}
}

@inproceedings{santoroWaleDockerfileBasedApproach2018,
  title = {Wale: {{A Dockerfile-Based Approach}} to {{Deduplicate Shared Libraries}} in {{Docker Containers}}},
  shorttitle = {Wale},
  booktitle = {2018 {{IEEE}} 16th {{Intl Conf}} on {{Dependable}}, {{Autonomic}} and {{Secure Computing}}, 16th {{Intl Conf}} on {{Pervasive Intelligence}} and {{Computing}}, 4th {{Intl Conf}} on {{Big Data Intelligence}} and {{Computing}} and {{Cyber Science}} and {{Technology Congress}}({{DASC}}/{{PiCom}}/{{DataCom}}/{{CyberSciTech}})},
  author = {Santoro, Corrado and Messina, Fabrizio and D'Urso, Fabio and Santoro, Federico Fausto},
  date = {2018-08},
  pages = {785--791},
  doi = {10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00135},
  abstract = {This paper presents a simple, novel approach to deduplicate disk space needed by shared application libraries in Docker containers. The described approach relies on the deployment of a single Docker image called "core-image", designed to collect all the libraries shared among the various Docker images. The management of the Docker images is made by means of the automatic manipulation of the related Dockerfile, i.e. the text files which describe the composition of Docker images. The user is asked to enrich its own Dockerfiles with a number of directives, as Dockerfile comments, that represent the meta-data needed to identify the common libraries among Docker images. In this way, common libraries and files are moved into the core-image, which is rebuilt again along with the new image containers. The latter will contain only libraries and files that are not shared. The paper includes the description of a simple but realistic case study which proves the effectiveness of the approach in terms of saved disk space.},
  eventtitle = {2018 {{IEEE}} 16th {{Intl Conf}} on {{Dependable}}, {{Autonomic}} and {{Secure Computing}}, 16th {{Intl Conf}} on {{Pervasive Intelligence}} and {{Computing}}, 4th {{Intl Conf}} on {{Big Data Intelligence}} and {{Computing}} and {{Cyber Science}} and {{Technology Congress}}({{DASC}}/{{PiCom}}/{{DataCom}}/{{CyberSciTech}})},
  keywords = {Buildings,Cloud computing,Cloud Computing,Container,Containers,Docker,File system,Libraries,Virtual machine monitors,Virtual machining,Virtualization},
  file = {/home/mechjm/Zotero/storage/52ZF866M/Santoro et al_2018_Wale.pdf;/home/mechjm/Zotero/storage/UDE4ETIP/stamp.html}
}

@article{sorgallaApplyingModelDrivenEngineering2021,
  title = {Applying {{Model-Driven Engineering}} to {{Stimulate}} the {{Adoption}} of {{DevOps Processes}} in {{Small}} and {{Medium-Sized Development Organizations}}: {{The Case}} for {{Microservice Architecture}}},
  shorttitle = {Applying {{Model-Driven Engineering}} to {{Stimulate}} the {{Adoption}} of {{DevOps Processes}} in {{Small}} and {{Medium-Sized Development Organizations}}},
  author = {Sorgalla, Jonas and Wizenty, Philip and Rademacher, Florian and Sachweh, Sabine and Zündorf, Albert},
  date = {2021-11},
  journaltitle = {SN Computer Science},
  shortjournal = {SN COMPUT. SCI.},
  volume = {2},
  number = {6},
  pages = {459},
  issn = {2662-995X, 2661-8907},
  doi = {10.1007/s42979-021-00825-z},
  url = {https://link.springer.com/10.1007/s42979-021-00825-z},
  urldate = {2022-11-05},
  abstract = {Microservice architecture (MSA) denotes an increasingly popular architectural style in which business capabilities are wrapped into autonomously developable and deployable software components called microservices. Microservice applications are developed by multiple DevOps teams each owning one or more services. In this article, we explore the state of how DevOps teams in small and medium-sized organizations (SMOs) cope with MSA and how they can be supported. We show through a secondary analysis of an exploratory interview study comprising six cases, that the organizational and technological complexity resulting from MSA poses particular challenges for small and medium-sized organizations (SMOs). We apply model-driven engineering to address these challenges. As results of the second analysis, we identify the challenge areas of building and maintaining a common architectural understanding, and dealing with deployment technologies. To support DevOps teams of SMOs in coping with these challenges, we present a model-driven workflow based on LEMMA—the Language Ecosystem for Modeling Microservice Architecture. To implement the workflow, we extend LEMMA with the functionality to (i) generate models from API documentation; (ii) reference remote models owned by other teams; (iii) generate deployment specifications; and (iv) generate a visual representation of the overall architecture. We validate the model-driven workflow and our extensions to LEMMA through a case study showing that the added functionality to LEMMA can bring efficiency gains for DevOps teams. To develop best practices for applying our workflow to maximize efficiency in SMOs, we plan to conduct more empirical research in the field in the future.},
  langid = {english},
  file = {/home/mechjm/Zotero/storage/CI7XYTIQ/Sorgalla et al. - 2021 - Applying Model-Driven Engineering to Stimulate the.pdf}
}

@inproceedings{tomassiBugSwarmMiningContinuously2019,
  title = {{{BugSwarm}}: {{Mining}} and {{Continuously Growing}} a {{Dataset}} of {{Reproducible Failures}} and {{Fixes}}},
  shorttitle = {{{BugSwarm}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Tomassi, David A. and Dmeiri, Naji and Wang, Yichen and Bhowmick, Antara and Liu, Yen-Chuan and Devanbu, Premkumar T. and Vasilescu, Bogdan and Rubio-González, Cindy},
  date = {2019-05},
  pages = {339--349},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2019.00048},
  abstract = {Fault-detection, localization, and repair methods are vital to software quality; but it is difficult to evaluate their generality, applicability, and current effectiveness. Large, diverse, realistic datasets of durably-reproducible faults and fixes are vital to good experimental evaluation of approaches to software quality, but they are difficult and expensive to assemble and keep current. Modern continuous-integration (CI) approaches, like TRAVIS-CI, which are widely used, fully configurable, and executed within custom-built containers, promise a path toward much larger defect datasets. If we can identify and archive failing and subsequent passing runs, the containers will provide a substantial assurance of durable future reproducibility of build and test. Several obstacles, however, must be overcome to make this a practical reality. We describe BUGSWARM, a toolset that navigates these obstacles to enable the creation of a scalable, diverse, realistic, continuously growing set of durably reproducible failing and passing versions of real-world, open-source systems. The BUGSWARM toolkit has already gathered 3,091 fail-pass pairs, in Java and Python, all packaged within fully reproducible containers. Furthermore, the toolkit can be run periodically to detect fail-pass activities, thus growing the dataset continually.},
  eventtitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  keywords = {Bug Database,Experiment Infrastructure,Java,Libraries,Open source software,Program Analysis,Python,Reproducibility,Software maintenance,Software quality,Software Testing,Tools},
  file = {/home/mechjm/Zotero/storage/3QSV7524/Tomassi et al_2019_BugSwarm.pdf;/home/mechjm/Zotero/storage/HNB2IFVI/8812141.html}
}

@article{tomyModusDatalogDialect2022,
  title = {Modus: {{A Datalog Dialect}} for {{Building Container Images}}},
  author = {Tomy, Chris and Wang, Tingmao and Barr, Earl T and Mechtaev, Sergey},
  date = {2022},
  pages = {12},
  abstract = {Containers help share and deploy software by packaging it with all its dependencies. Tools, like Docker or Kubernetes, spawn containers from images as specified by a build system’s language, such as Dockerfile. A build system takes many parameters to build an image, including OS and application versions. These build parameters can interact: setting one can restrict another. Dockerfile lacks support for reifying and constraining these interactions, thus forcing developers to write a build script per workflow. As a result, developers have resorted to creating ad hoc solutions such as templates or domain-specific frameworks that harm performance and complicate maintenance because they are verbose and mix languages.},
  langid = {english},
  file = {/home/mechjm/Zotero/storage/X2EKGXCV/Tomy et al. - 2022 - Modus A Datalog Dialect for Building Container Im.pdf}
}

@online{BuildKit2022,
  title = {{{BuildKit}}},
  date = {2022-12-29T15:07:23+00:00},
  url = {https://docs.docker.com/build/buildkit/},
  urldate = {2022-12-31},
  abstract = {Introduction and overview of BuildKit},
  langid = {english},
  organization = {{Docker Documentation}},
  file = {/home/mechjm/Zotero/storage/BWZJ7UKG/buildkit.html}
}

@misc{wangCodeInjectionMethod2019,
  title = {A {{Code Injection Method}} for {{Rapid Docker Image Building}}},
  author = {Wang, Yujing and Bao, Qinyang},
  date = {2019-11-25},
  number = {arXiv:1911.07444},
  eprint = {1911.07444},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1911.07444},
  url = {http://arxiv.org/abs/1911.07444},
  urldate = {2022-12-26},
  abstract = {Docker images are composed of multiple layers, each of which contains a set of instructions, and an archive of files. Layers allow Docker to separate a large build task into smaller ones, such that when a part of the program is changed, only the corresponding layer needs to be changed. Yet the current implementation has major inefficiencies that make the rebuilding of an image unnecessarily slow when changes in bottom layers are required: uneven content distribution amongst layers, the need to rebuild an entire layer during update, and the rebuild fall-throughs in many cases. In this paper, we propose a code injection method that overcomes these inefficiencies by targeting only the changed layer and then bypassing the layer's content checksum. This process is developed specifically for an interpreted language such as Python, where changes can be detected explicitly via text diff tools and run as-is without compilation. We then demonstrate that this method can accelerate the rebuild time, effectively reducing the O(n) where n = size of layer rebuild time to O(1). Whereas for compiled languages, literal code injection cannot guarantee integrity in compiled machine code. Expanding on the same code injection principle, multi-layer targeted code injection will be addressed in a future discussion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/home/mechjm/Zotero/storage/IYIHR45P/Wang_Bao_2019_A Code Injection Method for Rapid Docker Image Building.pdf;/home/mechjm/Zotero/storage/MAJU4G46/1911.html}
}

@misc{weberIdiomaticReproducibleSoftware2017,
  title = {Idiomatic and {{Reproducible Software Builds}} Using {{Containers}} for {{Reliable Computing}}},
  author = {Weber, Jonas},
  date = {2017-02-09},
  number = {arXiv:1702.02999},
  eprint = {1702.02999},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1702.02999},
  url = {http://arxiv.org/abs/1702.02999},
  urldate = {2022-12-28},
  abstract = {Containers as the unit of application delivery are the 'next big thing' in the software development world. They enable developers to create an executable image containing an application bundled with all its dependencies which a user can run inside a controlled environment with virtualized resources. Complex workflows for business-critical applications and research environments require a high degree of reproducibility which can be accomplished using uniquely identified images as units of computation. It will be shown in this thesis that the most widely used approaches to create an image from pre-existing software or from source code lack the ability to provide idiomaticity in their use of the technology as well as proper reproducibility safe-guards. In the first part, existing approaches are formalized and discussed and a new approach is introduced. The approaches are then evaluated using a suite of three different examples. This thesis provides a framework for formalizing operations involving a layered file system, containers and images, and a novel approach to the creation of images using utility containers and layer donning fulfilling the idiomaticity and reproducibility criteria.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering},
  file = {/home/mechjm/Zotero/storage/4IUHFT8G/Weber_2017_Idiomatic and Reproducible Software Builds using Containers for Reliable.pdf;/home/mechjm/Zotero/storage/WF3P8J9Y/1702.html}
}

@article{wuCharacterizingOccurrenceDockerfile2020,
  title = {Characterizing the {{Occurrence}} of {{Dockerfile Smells}} in {{Open-Source Software}}: {{An Empirical Study}}},
  shorttitle = {Characterizing the {{Occurrence}} of {{Dockerfile Smells}} in {{Open-Source Software}}},
  author = {Wu, Yiwen and Zhang, Yang and Wang, Tao and Wang, Huaimin},
  date = {2020},
  journaltitle = {IEEE Access},
  volume = {8},
  pages = {34127--34139},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2973750},
  abstract = {Dockerfile plays an important role in the Docker-based software development process, but many Dockerfile codes are infected with smells in practice. Understanding the occurrence of Dockerfile smells in open-source software can benefit the practice of Dockerfile and enhance project maintenance. In this paper, we perform an empirical study on a large dataset of 6,334 projects to help developers gain some insights into the occurrence of Dockerfile smells, including its coverage, distribution, co-occurrence, and correlation with project characteristics. Our results show that smells are very common in Dockerfile codes and there exists co-occurrence between different types of Dockerfile smells. Further, using linear regression analysis, when controlled for various variables, we statistically identify and quantify the relationships between Dockerfile smells occurrence and project characteristics. We also provide a rich resource of implications for software practitioners.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Best practices,Buildings,Containers,Databases,Docker,Dockerfile smells,GitHub,Open source software,Open-source software},
  file = {/home/mechjm/Zotero/storage/I5QTP275/Wu et al_2020_Characterizing the Occurrence of Dockerfile Smells in Open-Source Software.pdf;/home/mechjm/Zotero/storage/G2CN5LH6/8998208.html}
}

@inproceedings{wuDockerfileChangesPractice2020,
  title = {Dockerfile {{Changes}} in {{Practice}}: {{A Large-Scale Empirical Study}} of 4,110 {{Projects}} on {{GitHub}}},
  shorttitle = {Dockerfile {{Changes}} in {{Practice}}},
  booktitle = {2020 27th {{Asia-Pacific Software Engineering Conference}} ({{APSEC}})},
  author = {Wu, Yiwen and Zhang, Yang and Wang, Tao and Wang, Huaimin},
  date = {2020-12},
  pages = {247--256},
  issn = {2640-0715},
  doi = {10.1109/APSEC51365.2020.00033},
  abstract = {Docker is one of the most popular containerization tools in current DevOps practice. Particularly, Dockerfile plays an important role in the Docker-based software development process by specifying the commands and build environment of Docker containers. As a project progresses through its development stages, the content of the Dockerfile may be revised many times. Previous studies have examined Dockerfile usage in open-source projects. However, little is known about the details of Dockerfile changes in practice. In this paper, we conduct an empirical study on Dockerfile changes for 4,110 open-source projects hosted on GitHub. Based on the Dockerfile data, we measure the frequency, magnitude, and instructions of Dockerfile changes and report how Dockerfile co-changed with other files. To explore the relationship between Dockerfile changes and project outcomes, i.e., popularity, success, and productivity, we also develop regression models, by controlling for various confounds. Our findings help to characterize and understand Dockerfile changes and motivate the need for collecting more empirical evidence.},
  eventtitle = {2020 27th {{Asia-Pacific Software Engineering Conference}} ({{APSEC}})},
  keywords = {DevOps practice,Dockerfile,Dockerfile changes,Empirical study,Frequency measurement,Maintenance engineering,Open source software,Open-Source,Productivity,Software development management,Software engineering,Tools},
  file = {/home/mechjm/Zotero/storage/EHZIYRP2/Wu et al. - 2020 - Dockerfile Changes in Practice A Large-Scale Empi.pdf;/home/mechjm/Zotero/storage/6V3TC6EX/9359307.html}
}

@inproceedings{wuEmpiricalStudyBuild2020,
  title = {An {{Empirical Study}} of {{Build Failures}} in the {{Docker Context}}},
  booktitle = {Proceedings of the 17th {{International Conference}} on {{Mining Software Repositories}}},
  author = {Wu, Yiwen and Zhang, Yang and Wang, Tao and Wang, Huaimin},
  date = {2020-06-29},
  pages = {76--80},
  publisher = {{ACM}},
  location = {{Seoul Republic of Korea}},
  doi = {10.1145/3379597.3387483},
  url = {https://dl.acm.org/doi/10.1145/3379597.3387483},
  urldate = {2022-11-05},
  abstract = {Docker containers have become the de-facto industry standard. Docker builds often break, and a large amount of efforts are put into troubleshooting broken builds. Prior studies have evaluated the rate at which builds in large organizations fail. However, little is known about the frequency and fix effort of failures that occur in Docker builds of open-source projects. This paper provides a first attempt to present a preliminary study on 857,086 Docker builds from 3,828 open-source projects hosted on GitHub. Using the Docker build data, we measure the frequency of broken builds and report their fix time. Furthermore, we explore the evolution of Docker build failures across time. Our findings help to characterize and understand Docker build failures and motivate the need for collecting more empirical evidence.},
  eventtitle = {{{MSR}} '20: 17th {{International Conference}} on {{Mining Software Repositories}}},
  isbn = {978-1-4503-7517-7},
  langid = {english},
  file = {/home/mechjm/Zotero/storage/QWM5CMDG/Wu et al. - 2020 - An Empirical Study of Build Failures in the Docker.pdf}
}

@inproceedings{xuDockerfileTFSmell2019,
  title = {Dockerfile {{TF Smell Detection Based}} on {{Dynamic}} and {{Static Analysis Methods}}},
  booktitle = {2019 {{IEEE}} 43rd {{Annual Computer Software}} and {{Applications Conference}} ({{COMPSAC}})},
  author = {Xu, Jiwei and Wu, Yuewen and Lu, Zhigang and Wang, Tao},
  date = {2019-07},
  volume = {1},
  pages = {185--190},
  issn = {0730-3157},
  doi = {10.1109/COMPSAC.2019.00033},
  abstract = {Dockerfile is used to build docker image. In the image building process, temporary files are frequently used to import applications and data. A careless use of Dockerfile may cause temporary file left in the image, which can increase the image size, thus effects the elastic scale ability and QoS. This problem is identified as temporary file smell. The feature of UnionFS that docker image used is different from traditional filesystems. If users are not paying attention, they are too apt to make such mistakes. To address this problem, we propose two different methods to detect temporary file smell with dynamic analysis and static analysis respectively. We use the really-world cases to evaluate our methods. Experimental results show that our methods can effectively detect the temporary file smell.},
  eventtitle = {2019 {{IEEE}} 43rd {{Annual Computer Software}} and {{Applications Conference}} ({{COMPSAC}})},
  keywords = {Buildings,Containers,Dockerfile; temporary file smell; dynamic analysis; static analysis,Kernel,Linux,Static analysis,Syntactics},
  file = {/home/mechjm/Zotero/storage/ABJ4SQW5/Xu et al_2019_Dockerfile TF Smell Detection Based on Dynamic and Static Analysis Methods.pdf;/home/mechjm/Zotero/storage/9I6IJFYA/8753910.html}
}

@inproceedings{yeDockerGenKnowledgeGraph2021,
  title = {{{DockerGen}}: {{A Knowledge Graph}} Based {{Approach}} for {{Software Containerization}}},
  shorttitle = {{{DockerGen}}},
  booktitle = {2021 {{IEEE}} 45th {{Annual Computers}}, {{Software}}, and {{Applications Conference}} ({{COMPSAC}})},
  author = {Ye, Hongjie and Zhou, Jiahong and Chen, Wei and Zhu, Jiaxin and Wu, Guoquan and Wei, Jun},
  date = {2021-07},
  pages = {986--991},
  issn = {0730-3157},
  doi = {10.1109/COMPSAC51774.2021.00133},
  abstract = {Docker is the de-facto container technology for software system deployment and delivery. A Dockerfile specifies how to containerize a system into a Docker image. However, creating a Dockerfile is not trivial since resolving the dependencies (e.g., third-party libraries) of diverse software requires comprehensive domain knowledge. In this paper, we propose DockerGen to containerize software packages automatically. DockerGen constructs a knowledge graph containing rich knowledge of building Docker images by analyzing nearly 220 thousand Dockerfiles. DockerGen exploits the knowledge graph to containerize the target software by creating a Dockerfile specifying the base image, dependencies, and the operation workflow. We evaluate DockerGen on 100 software packages of various categories. DockerGen achieves a 73\% build success rate and a 59\% configuration success rate. The experimental result indicates it is viable to automate software containerization based on a domain knowledge graph.},
  eventtitle = {2021 {{IEEE}} 45th {{Annual Computers}}, {{Software}}, and {{Applications Conference}} ({{COMPSAC}})},
  keywords = {Conferences,containerization knowledge graph,Containers,dependency,Docker,Dockerfile,Image resolution,Libraries,software package,Software packages,Software systems,Writing},
  file = {/home/mechjm/Zotero/storage/4B74TLIX/Ye et al_2021_DockerGen.pdf;/home/mechjm/Zotero/storage/Q8AGGNZA/9529714.html}
}

@article{zareiInvestigatingInnerWorkings2022,
  title = {Investigating the Inner Workings of Container Image Vulnerability Scanners},
  author = {Zarei, Mehdi},
  date = {2022},
  pages = {96},
  url = {https://oda.oslomet.no/oda-xmlui/bitstream/handle/11250/3017416/zarei-acit2022.pdf?sequence=1},
  urldate = {2022-11-14},
  abstract = {The use of container technology as a main part of software development increasing exponentially. Containers do not only provide a huge benefit for Integration/Continuous Delivery (CI/CD) pipelines, but also simplify shipping problems. However, the security of container images is a primary concern. Exploitation of a single vulnerability in an image could have huge consequences and result in loss of CIA (Confidentiality, Integrity, Availability) in an application. While there are a variety of image scanners that create vulnerability reports informing the security teams, there is a lack of knowledge about the inner workings of container images and how they interact with different types of images. First, this thesis describes the history of containers, tools, and techno- logy related to containers. Second, we discuss some of the most popu- lar container image scanners and have selected two which are both open- source and highly ranked. Next, the thesis explains how scanners detect packages and vulnerabilities. Finally, a few experiments are conducted with three different types of containers; standard container images, dis- troless and images that have been slimmed down. These kinds of images are scanned using the image scanners and the results are compared. Our findings reveal that: 1. Both selected images scanners use roughly the same algorithm to detect vulnerabilities 2. Trivy supports more OS and application packages 3. The majority of the detected vulnerabilities are unfixed vulnerabilit- ies 4. None of the tested scanners were able to detect vulnerabilities when using slimmed down images.},
  file = {/home/mechjm/Zotero/storage/GL7KWTXY/zarei-acit2022.pdf}
}

@article{zhongBurnerRecipeAutomatic2022,
  title = {Burner: {{Recipe Automatic Generation}} for {{HPC Container Based}} on {{Domain Knowledge Graph}}},
  shorttitle = {Burner},
  author = {Zhong, Shuaihao and Wang, Duoqiang and Li, Wei and Lu, Feng and Jin, Hai},
  date = {2022-05-25},
  journaltitle = {Wireless Communications and Mobile Computing},
  volume = {2022},
  pages = {e4592428},
  publisher = {{Hindawi}},
  issn = {1530-8669},
  doi = {10.1155/2022/4592428},
  url = {https://www.hindawi.com/journals/wcmc/2022/4592428/},
  urldate = {2022-12-29},
  abstract = {As one of the emerging cloud computing technologies, containers are widely used in academia and industry. The cloud computing built by the container in the high performance computing (HPC) center can provide high-quality services to users at the edge. Singularity Definition File and Dockerfile (we refer to such files as recipes) have attracted wide attention due to their encapsulation of the application running environment in a container. However, creating a recipe requires extensive domain knowledge, which is error-prone and time-consuming. Accordingly, more than 34\% of Dockerfiles in Github cannot successfully build container images. The crucial points about recipe creation include selecting the entities (base images and packages) and determining their relationships (correct installation order for transitive dependencies). Since the relationships between entities can be expressed accurately and efficiently by the knowledge graph, we introduce knowledge graph to generate high-quality recipes automatically. This paper proposes an automatic recipe generation system named Burner, enabling users with no professional computer background to generate the recipes. We first develop a toolset including a recipe parser and an entity-relationship miner. Our two-phase recipe parsing method can perform abstract syntax tree (AST) parsing more deeply on the recipe file to achieve entity extraction; the parsing success rate (PSR) of the two-phase parsing method is 10.1\% higher than the one-phase parsing. Then, we build a knowledge base containing 2,832 entities and 62,614 entity relationships, meeting the needs of typical HPC applications. In the test of image build, the singularity image build success rate reaches 80\%. Compared with the ItemCF recommendation method, our recommendation method TB-TFIDF achieves a performance improvement by up to 50.86\%.},
  langid = {english},
  file = {/home/mechjm/Zotero/storage/2J2Q7CNH/Zhong et al_2022_Burner.pdf}
}

@inproceedings{zhouDockerKGKnowledgeGraph2020,
  title = {{{DockerKG}}: {{A Knowledge Graph}} of {{Docker Artifacts}}},
  shorttitle = {{{DockerKG}}},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} 42nd {{International Conference}} on {{Software Engineering Workshops}}},
  author = {Zhou, Jiahong and Chen, Wei and Liu, Chang and Zhu, Jiaxin and Wu, Guoquan and Wei, Jun},
  date = {2020-09-25},
  series = {{{ICSEW}}'20},
  pages = {367--372},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3387940.3392161},
  url = {https://doi.org/10.1145/3387940.3392161},
  urldate = {2022-11-21},
  abstract = {Docker helps developers reuse software artifacts by providing a lightweight solution to the problem of operating system virtualization. A Docker image contains very rich and useful knowledge of software engineering, including the source of software packages, the correlations among software packages, the installation methods of software packages and the information on operating systems. To effectively obtain this knowledge, this paper proposes an approach to constructing a knowledge graph of Docker artifacts, named DockerKG, by analyzing a large number of Dockerfiles in Docker Hub, which contains more than 3.08 million Docker repositories (up to February 2020). Currently, DockerKG contains the domain knowledge extracted from approximately 200 thousand Dockerfiles in Docker Hub. Besides, it contains the information on Docker repositories and their semantic tags. In future work, DockerKG can be used for Docker image recommendations and online Q\&A service providing software engineering domain knowledge.},
  isbn = {978-1-4503-7963-2},
  keywords = {Docker,Dockerfile,knowledge graph,software package},
  file = {/home/mechjm/Zotero/storage/NGYHBLHD/Zhou et al_2020_DockerKG.pdf}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }
